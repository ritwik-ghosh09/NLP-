{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devbox/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import  FAISS\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from pathlib import Path\n",
    "\n",
    "summary_md_path = Path(\"/home/devbox/test/PR Lab/data/Summary_md\")\n",
    "md_parser = MarkdownNodeParser()\n",
    "doc_summary_list = []\n",
    "for md_summary in summary_md_path.glob('*.md'):\n",
    "    with open(md_summary, 'r') as file:\n",
    "        content = file.read()\n",
    "    filename = Path(md_summary).stem\n",
    "    doc_summary_list.extend(Document(page_content=content,\n",
    "                                    metadata={\"file_name\": filename[:-12], \"year\": filename[-11:-7], \"summary\": True}\n",
    "                            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks : 184\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "chunks_md_path = Path(\"/home/devbox/test/PR Lab/data/Markdowns\")\n",
    "all_chunks_list = []\n",
    "\n",
    "md_splitter = MarkdownHeaderTextSplitter([(\"#\", \"header_1\"),\n",
    "                                          (\"##\", \"header_2\")])\n",
    "''',\n",
    "                                          (\"##\", \"header_2\"),\n",
    "                                          (\"###\", \"header_3\")'''\n",
    "\n",
    "for md_file in chunks_md_path.glob('*.md'):\n",
    "    filename = Path(md_file).stem\n",
    "    if filename[:-7] == \"wsge_vampyr_1732\":\n",
    "        with open(md_file, 'r') as file:\n",
    "            content = file.read()\n",
    "        doc_chunks= md_splitter.split_text(content)\n",
    "        [doc_chunks[i].metadata.update({\n",
    "            \"file_name\": filename[:-12],\n",
    "            \"year\": filename[-11:-7],\n",
    "            \"summary\": False\n",
    "        }) for i in range(len(doc_chunks))]\n",
    "        all_chunks_list.extend(doc_chunks)  #extends all chunks of one markdown file with those of another\n",
    "print(f'Total Chunks : {len(all_chunks_list)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_chunks_list)):\n",
    "    header1 = all_chunks_list[i].metadata['header_1']\n",
    "    print(f'\\n---------------------------- Chunk {i+1} ----------------------------------\\n')\n",
    "    print(f'Header 1: {header1}')\n",
    "    if 'header_2' in all_chunks_list[i].metadata.keys():\n",
    "        print(f'Header 2: {all_chunks_list[i].metadata['header_2']}')\n",
    "    print(f'Content: \\n        {all_chunks_list[i].page_content}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Chunking as an alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-de and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks of dohm_frauen_erziehung_1910: 49\n",
      "Total Chunks of kraft_medizinmann_1896: 37\n",
      "Total Chunks of wsge_vampyr_1732: 150\n",
      "Total Chunks of kirchner_bekentnis_1569: 44\n",
      "Total Chunks of bergmann_transfusion_1883: 28\n",
      "Total Chunks of morgenstern_studium_1888: 35\n",
      "Total Chunks of wulff_frau_1911: 5\n",
      "Total Chunks of schulze_zimmer_1911: 27\n",
      "Total Chunks of humboldt_geognostisch_1838: 42\n",
      "Total Chunks of hannover_feurordnung_1681: 19\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpointEmbeddings\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import pandas as pd\n",
    "\n",
    "model_stella = \"dunzhang/stella_en_1.5B_v5\" #5.7 GB\n",
    "model_bge_en = \"BAAI/bge-en-icl\"    # 26.5 GB   32k tokens\n",
    "model_jina_v2 = \"jinaai/jina-embeddings-v2-base-de\" # 0.6GB    8k tokens\n",
    "model_jina_v3 = \"jinaai/jina-embeddings-v3\"         # 2.13 GB  8k tokens\n",
    "model_gbert = \"deepset/gbert-large\" # 1.26 GB   512 tokens\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_jina_v2)  #jinaai/jina-embeddings-v3\n",
    "#embedding_model = HuggingFaceInferenceAPIEmbeddings(api_key=\"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\", \n",
    " #                                                 model_name=\"BAAI/bge-en-icl\") #dunzhang/stella_en_1.5B_v5\n",
    "semantic_splitter = SemanticChunker(embeddings=embedding_model,\n",
    "                                    buffer_size=3,\n",
    "                                    min_chunk_size=300,\n",
    "                                    #sentence_split_regex=r\".{7}(?=tes Capitel)\",\n",
    "                                    breakpoint_threshold_type='gradient',\n",
    "                                    breakpoint_threshold_amount=85)\n",
    "text_path = Path('/home/devbox/test/PR Lab/data/Raw')\n",
    "text = ''\n",
    "for text_file in text_path.glob('*.txt'):\n",
    "    #if text_file.stem == 'morgenstern_studium_1888.TEI-P5':\n",
    "    file_name = text_file.stem[:-7]\n",
    "    with open(text_file, 'r') as file:\n",
    "        content = file.read()\n",
    "    semantic_chunks = semantic_splitter.split_text(content)\n",
    "    df = pd.DataFrame(semantic_chunks, columns=['Semantic_chunks'])\n",
    "    df.to_csv(f\"/home/devbox/test/PR Lab/data/Semantic_chunks/{file_name}.csv\", index=False)\n",
    "    print(f\"Total Chunks of {text_file.stem[:-7]}: {len(semantic_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(semantic_chunks))\n",
    "for idx, chunk in enumerate(semantic_chunks):\n",
    "    #key_vals = chunk.metadata.keys()\n",
    "    print(f'\\n----------------------- Chunk {idx+1} ----------------------------\\n')\n",
    "    '''if 'header_1' in key_vals:\n",
    "        print(f'Header 1: {header_1}')'''\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Summaries from splitted chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "import langchain\n",
    "\n",
    "langchain.verbose = False\n",
    "langchain.debug = False\n",
    "langchain.llm_cache = False\n",
    "qwen = ChatOpenAI(model=\"qwen2:72b\", \n",
    "        base_url=\"http://localhost:11434/v1\", \n",
    "        temperature=0.2,\n",
    "        api_key=\"NA\")\n",
    "llama = ChatOpenAI(model=\"llama3.1:70b\", \n",
    "                        base_url=\"http://localhost:11434/v1\", \n",
    "                        temperature=0.2,\n",
    "                        api_key=\"NA\")\n",
    "\n",
    "def gen_summary_map(model, documents):\n",
    "    map_prompt = \"\"\"\n",
    "You will be given a single passage of a German book on history and old german text. This section will be enclosed under PASSAGE within double quotes (\"\")\n",
    "Your goal is to give a concise summary of this section so that a reader will have a full understanding of what happened just by looking at the summary.\n",
    "Your response should capture the main context and fully encompass what was said in the passage.\n",
    "\n",
    "PASSAGE:\n",
    "\"{text}\"\n",
    "\n",
    "CONCISE SUMMARY:\n",
    "\"\"\"\n",
    "    map_prompt_template = PromptTemplate.from_template(template=map_prompt) #, input_variables=[\"text\"])\n",
    "\n",
    "    combine_prompt = \"\"\"\n",
    "Write a concise summary of the following text delimited by triple backquotes.\n",
    "Return your response in bullet points which covers the key points of the text.\n",
    "```{text}```\n",
    "BULLET POINT SUMMARY:\n",
    "\"\"\"\n",
    "    combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n",
    "\n",
    "    refine_prompt = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in German\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    "    )\n",
    "    refine_prompt_template = PromptTemplate.from_template(template=refine_prompt) #input_variables=[\"existing_answer\", \"text\"])\n",
    "    if model == 'qwen':\n",
    "        llm = qwen\n",
    "    else:\n",
    "        llm = llama\n",
    "    summary_chain = load_summarize_chain(llm=llm,\n",
    "                                         chain_type=\"refine\",\n",
    "                                         #map_prompt=map_prompt_template,\n",
    "                                         #combine_prompt=combine_prompt_template,\n",
    "                                         #input_key=\"input_documents\",\n",
    "                                         #output_key=\"output_text\"\n",
    "                                         #verbose=True\n",
    "                                         )\n",
    "    result = summary_chain.invoke(documents)\n",
    "\n",
    "    return result['output_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devbox/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
      "/home/devbox/miniconda3/lib/python3.12/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n",
      "/home/devbox/miniconda3/lib/python3.12/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Literal, TypedDict\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen2:72b\", \n",
    "        base_url=\"http://localhost:11434/v1\", \n",
    "        temperature=0.2,\n",
    "        api_key=\"NA\")\n",
    "# Initial summary\n",
    "summarize_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"human\", \"Write a concise summary of the following: {context}\"),\n",
    "    ]\n",
    ")\n",
    "initial_summary_chain = summarize_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Refining the summary with new docs\n",
    "refine_template = \"\"\"\n",
    "Produce a final summary.\n",
    "\n",
    "Existing summary up to this point:\n",
    "{existing_answer}\n",
    "\n",
    "New context:\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original summary.\n",
    "\"\"\"\n",
    "refine_prompt = ChatPromptTemplate([(\"human\", refine_template)])\n",
    "\n",
    "refine_summary_chain = refine_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# We will define the state of the graph to hold the document\n",
    "# contents and summary. We also include an index to keep track\n",
    "# of our position in the sequence of documents.\n",
    "class State(TypedDict):\n",
    "    contents: List[str]\n",
    "    index: int\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# We define functions for each node, including a node that generates\n",
    "# the initial summary:\n",
    "async def generate_initial_summary(state: State, config: RunnableConfig):\n",
    "    summary = await initial_summary_chain.ainvoke(\n",
    "        state[\"contents\"][0],\n",
    "        config,\n",
    "    )\n",
    "    return {\"summary\": summary, \"index\": 1}\n",
    "\n",
    "\n",
    "# And a node that refines the summary based on the next document\n",
    "async def refine_summary(state: State, config: RunnableConfig):\n",
    "    content = state[\"contents\"][state[\"index\"]]\n",
    "    summary = await refine_summary_chain.ainvoke(\n",
    "        {\"existing_answer\": state[\"summary\"], \"context\": content},\n",
    "        config,\n",
    "    )\n",
    "\n",
    "    return {\"summary\": summary, \"index\": state[\"index\"] + 1}\n",
    "\n",
    "\n",
    "# Here we implement logic to either exit the application or refine\n",
    "# the summary.\n",
    "def should_refine(state: State) -> Literal[\"refine_summary\", END]:\n",
    "    if state[\"index\"] >= len(state[\"contents\"]):\n",
    "        return END\n",
    "    else:\n",
    "        return \"refine_summary\"\n",
    "\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"generate_initial_summary\", generate_initial_summary)\n",
    "graph.add_node(\"refine_summary\", refine_summary)\n",
    "\n",
    "graph.add_edge(START, \"generate_initial_summary\")\n",
    "graph.add_conditional_edges(\"generate_initial_summary\", should_refine)\n",
    "graph.add_conditional_edges(\"refine_summary\", should_refine)\n",
    "app = graph.compile()\n",
    "\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FieldInfo' object is not a mapping",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     23\u001b[0m documentss \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApples are red\u001b[39m\u001b[38;5;124m\"\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapple_book\u001b[39m\u001b[38;5;124m\"\u001b[39m}),\n\u001b[1;32m     25\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlueberries are blue\u001b[39m\u001b[38;5;124m\"\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblueberry_book\u001b[39m\u001b[38;5;124m\"\u001b[39m}),\n\u001b[1;32m     26\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBananas are yelow\u001b[39m\u001b[38;5;124m\"\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbanana_book\u001b[39m\u001b[38;5;124m\"\u001b[39m}),\n\u001b[1;32m     27\u001b[0m ]\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m     29\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m: [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documentss]},\n\u001b[1;32m     30\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m ):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summary \u001b[38;5;241m:=\u001b[39m step\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1524\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1519\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1520\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1521\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1522\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1523\u001b[0m ):\n\u001b[0;32m-> 1524\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1525\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1526\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1527\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1528\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1529\u001b[0m     ):\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1532\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/pregel/runner.py:130\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    128\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(t, retry_policy, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/pregel/retry.py:102\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/utils/runnable.py:453\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     coro \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langgraph/utils/runnable.py:236\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    235\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 236\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mgenerate_initial_summary\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_initial_summary\u001b[39m(state: State, config: RunnableConfig):\n\u001b[0;32m---> 55\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m initial_summary_chain\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m     56\u001b[0m         state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     57\u001b[0m         config,\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: summary, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_core/runnables/base.py:3066\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m     part \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(step\u001b[38;5;241m.\u001b[39mainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[0;32m-> 3066\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part(), context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3068\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    306\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 307\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    308\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    309\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    310\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    313\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    314\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:796\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    790\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    795\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    797\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    798\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:695\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate\u001b[39m(\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    661\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[BaseMessage]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    670\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously pass a sequence of prompts to a model and return generations.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \n\u001b[1;32m    673\u001b[0m \u001b[38;5;124;03m    This method should make use of batched calls for models that expose a batched\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m            prompt and additional model provider-specific output.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_invocation_params(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    696\u001b[0m     options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop}\n\u001b[1;32m    697\u001b[0m     inheritable_metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(metadata \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ls_params(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    700\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:560\u001b[0m, in \u001b[0;36mChatOpenAI._get_invocation_params\u001b[0;34m(self, stop, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_invocation_params\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m, stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    556\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    557\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the parameters used to invoke the model.\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[0;32m--> 560\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_get_invocation_params(stop\u001b[38;5;241m=\u001b[39mstop),\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params,\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    563\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:510\u001b[0m, in \u001b[0;36mBaseChatModel._get_invocation_params\u001b[0;34m(self, stop, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_invocation_params\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 510\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict()\n\u001b[1;32m    511\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1104\u001b[0m, in \u001b[0;36mBaseChatModel.dict\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a dictionary of the LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m     starter_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_identifying_params)\n\u001b[1;32m   1105\u001b[0m     starter_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm_type\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m starter_dict\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:552\u001b[0m, in \u001b[0;36mChatOpenAI._identifying_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_identifying_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the identifying parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:365\u001b[0m, in \u001b[0;36mChatOpenAI._default_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    364\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the default parameters for calling OpenAI API.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming,\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn,\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs,\n\u001b[1;32m    371\u001b[0m     }\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens\n",
      "\u001b[0;31mTypeError\u001b[0m: 'FieldInfo' object is not a mapping"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from pathlib import Path\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "semantic_dir = '/home/devbox/test/PR Lab/data/Semantic_chunks'\n",
    "text_path = Path(semantic_dir)\n",
    "text = ''\n",
    "count = 0\n",
    "for csv_file in text_path.glob('*.csv'):\n",
    "    #if text_file.stem == 'morgenstern_studium_1888.TEI-P5':\n",
    "    if count == 0:\n",
    "        file_name = csv_file.stem\n",
    "        docs = CSVLoader(file_path=f\"{semantic_dir}/{file_name}.csv\").load()\n",
    "        '''print(type(docs[0]))\n",
    "        final_summary = gen_summary_map(qwen, docs)\n",
    "        count += 1\n",
    "        print(type(final_summary))\n",
    "        print(final_summary)'''\n",
    "        \n",
    "\n",
    "\n",
    "documentss = [\n",
    "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
    "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
    "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
    "]\n",
    "async for step in app.astream(\n",
    "    {\"contents\": [doc.page_content for doc in documentss]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    if summary := step.get(\"summary\"):\n",
    "        print(summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
