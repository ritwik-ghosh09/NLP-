{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'instruction', 'instruction_score', 'response', 'response_score'],\n",
       "        num_rows: 639\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "smt_instr_dataset = load_dataset(\"ritwik-ghosh/SMT_Fine-tuning_dataset\")\n",
    "smt_instr_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Sample-length distribution across the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers : 32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "def plot_sequence_lengths(dataset_obj, bin_size):\n",
    "\n",
    "    # Initializing a list to store the sequence lengths\n",
    "    sequence_lengths = []\n",
    "\n",
    "    # list of indices that are too long\n",
    "    too_long = []\n",
    "\n",
    "    # Looping over the dataset and to the lengths of text sequences\n",
    "    for idx, example in enumerate(dataset_obj[\"train\"]):\n",
    "        sequence_lengths.append(len(example['Context']) + len(example[\"instruction\"]) + len(example[\"response\"]))\n",
    "        # filters out outliers\n",
    "        if sequence_lengths[idx] > 8000:\n",
    "          too_long.append(idx)\n",
    "    \n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(sequence_lengths, bins=bin_size)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Sample Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    return too_long\n",
    "indexes_to_drop = plot_sequence_lengths(smt_instr_dataset, 400)\n",
    "print(f\"Number of outliers : {len(indexes_to_drop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'instruction', 'instruction_score', 'response', 'response_score'],\n",
       "        num_rows: 546\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'instruction', 'instruction_score', 'response', 'response_score'],\n",
       "        num_rows: 61\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt_instr_dataset_norm = smt_instr_dataset[\"train\"].select(i for i in range(len(smt_instr_dataset[\"train\"])) if i not in set(indexes_to_drop))\n",
    "smt_instr_dataset_norm\n",
    "smt_instr_dataset_train_test = smt_instr_dataset_norm.train_test_split(test_size=0.1)\n",
    "plot_sequence_lengths(smt_instr_dataset_train_test, 250)\n",
    "smt_instr_dataset_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Format of the Instruction-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 546/546 [00:00<00:00, 9242.62 examples/s]\n",
      "Map: 100%|| 61/61 [00:00<00:00, 6874.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'instruction', 'instruction_score', 'response', 'response_score', 'text'],\n",
       "        num_rows: 546\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'instruction', 'instruction_score', 'response', 'response_score', 'text'],\n",
       "        num_rows: 61\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "  if example.get(\"context\", \"\") != \"\":\n",
    "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context.\"\n",
    "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "      \"### Instruction:\\n\"\n",
    "      f\"{example['instruction']}\\n\\n\"\n",
    "      f\"### Input: \\n\"\n",
    "      f\"{example['Context']}\\n\\n\"\n",
    "      f\"### Response: \\n\"\n",
    "      f\"{example['response']}\")\n",
    "\n",
    "  else:\n",
    "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
    "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "      \"### Instruction:\\n\"\n",
    "      f\"{example['instruction']}\\n\\n\"\n",
    "      f\"### Response:\\n\"\n",
    "      f\"{example['response']}\")\n",
    "\n",
    "  return {\"text\" : input_prompt}\n",
    "formatted_dataset = smt_instr_dataset_train_test.map(formatting_func)\n",
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Context': 'Figure 1 Status and developments for soft solders addressing elevated temperatures\\n\\n|180 C < T liquidus < 280 C| | | |\\n|---|---|---|---|\\n| |Sn-based solders|SAC xxTuse< 130C| |\\n|Developments:| |Au, Al, Zn-based solders|Different advantages/shortcomings T use< 200-300C|\\n| |Highly creep resistant Sn based,|e.g. Innolot, HT1,|Sn-Ag-Cu-Bi-Sb-Ni, Tuse< 160C|\\n\\nThe latter properties allow their use at elevated temperatures, as their fatigue properties are improved. They are already in series production use. However, a unique replacement for high-lead solder is not yet in sight. Other technologies, which address not only a simple replacement, but also offer new high temperature properties in terms of creep and fatigue resistance are under development. As shown in Figure 2, Ag-sintering and transient liquid phase (TLP) soldering need also to be mentioned.\\n\\nUsing an interlayer is one major characteristic of the TLP bonding process. In a difference from pure diffusion bonding, the interlayer melts and the interlayer element diffuses into the adjacent materials, causing isothermal solidification. The result of this process is a bond that has a higher melting point than the bonding temperature. Different metallic constituents can be used, however, the Sn-Cu system considered in this paper is one popular choice. The high-melting temperature of the interconnection consisting of Cu Sn5 and Cu3Sn IMCs is achieved by phase transformation of the Sn-Cu system.\\n\\nOne major application area of the technology in electronics is in the production of large joints, e.g. chip attach or chip stacking, where a thin soft-solder layer is on one part (mostly Sn), and a high-melting metallic layer on the other part, in most cases in terms of a Cu-pad or metallization. IMCs grow through the gap. This technological variant is often referred to as TLP bonding and is one of the upcoming technologies applied in power electronics interconnection technology.\\n\\nAnother technological approach is based on the creation of a composite solder paste. Processing time is one major concern in TLP soldering. One opportunity to accelerate the diffusion processes occurring would be an increase in the temperature of the melt, but this would lead to manufacturing problems and damage to the used parts. Another opportunity to obtain a large amount of intermetallic phases in a short time is via an increase in the diffusion contributing surfaces. Thus, the effective conversion of the melt is increased in solids when the diffusion path is shortened. Both an increase in the reaction surface area and diffusion path shortening is achieved in that a metal powder is used.\\n\\nEutectic SnCu0.7 solder was chosen for the soft solder part with Cu particles embedded. The new joining material(s) should enable an increase in the thermal stability by forming thermally stable intermetallic phases during the molten phase of the solder as a result of diffusion processes. Owing to isothermal solidification in a Cu-Sn diffusion system in which copper and tin are in contact in the temperature range up to 300C, from a thermodynamic perspective only (Cu), Cu6Sn5, Cu3Sn and (Sn) can form.',\n",
       " 'instruction': 'What are the advantages of using the Sn-Cu system in transient liquid phase (TLP) soldering, particularly in relation to the formation of intermetallic compounds (IMCs) and their impact on the thermal stability of the joint? Additionally, how does the incorporation of metal powder influence the diffusion processes and the overall effectiveness of the TLP bonding technology in power electronics interconnections?',\n",
       " 'instruction_score': 70.625,\n",
       " 'response': \"The Sn-Cu system utilized in transient liquid phase (TLP) soldering offers several notable advantages, particularly in the context of intermetallic compound (IMC) formation and the enhancement of thermal stability in solder joints. One of the primary benefits of employing the Sn-Cu system is its ability to facilitate the creation of thermally stable IMCs, such as Cu6Sn5 and Cu3Sn, during the soldering process. These compounds are essential as they significantly improve both the mechanical properties and thermal stability of the joint, which is crucial for applications that operate under elevated temperatures, such as in power electronics.\\n\\nThe TLP soldering process involves isothermal solidification, which occurs when copper and tin are in contact at temperatures up to 300C. This interaction promotes the growth of IMCs across the interface between the solder and the substrate, resulting in a bond that possesses a melting point higher than the bonding temperature itself. This characteristic is particularly advantageous for applications requiring robust thermal and mechanical performance, as it ensures reliable connections in high-temperature environments.\\n\\nIn addition to the inherent advantages of the Sn-Cu system, the incorporation of metal powder into the solder paste significantly influences the diffusion processes and overall effectiveness of TLP bonding technology. The presence of metal powder increases the reaction surface area, which accelerates the diffusion processes necessary for IMC formation. By shortening the diffusion path, the rate at which intermetallic phases develop is enhanced, leading to a more efficient bonding process. This is particularly critical in power electronics interconnections, where large joints, such as chip attach or chip stacking, require dependable and thermally stable connections.\\n\\nFurthermore, the strategic use of metal powder not only facilitates faster IMC formation but also contributes to the creation of a composite solder paste that can improve the thermal stability of the joint. The thermally stable intermetallic phases formed during the molten phase of the solder are a direct result of these diffusion processes, which are optimized by the presence of metal powder.\\n\\nIn summary, the combination of the Sn-Cu system's ability to form stable IMCs and the incorporation of metal powder to enhance diffusion processes positions TLP soldering as a promising technology for high-performance electronic applications. This synergy not only improves the thermal stability of the joints but also ensures their mechanical integrity under demanding operational conditions, making it a vital approach in the advancement of power electronics interconnection technology.\",\n",
       " 'response_score': 80.625,\n",
       " 'text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the advantages of using the Sn-Cu system in transient liquid phase (TLP) soldering, particularly in relation to the formation of intermetallic compounds (IMCs) and their impact on the thermal stability of the joint? Additionally, how does the incorporation of metal powder influence the diffusion processes and the overall effectiveness of the TLP bonding technology in power electronics interconnections?\\n\\n### Response:\\nThe Sn-Cu system utilized in transient liquid phase (TLP) soldering offers several notable advantages, particularly in the context of intermetallic compound (IMC) formation and the enhancement of thermal stability in solder joints. One of the primary benefits of employing the Sn-Cu system is its ability to facilitate the creation of thermally stable IMCs, such as Cu6Sn5 and Cu3Sn, during the soldering process. These compounds are essential as they significantly improve both the mechanical properties and thermal stability of the joint, which is crucial for applications that operate under elevated temperatures, such as in power electronics.\\n\\nThe TLP soldering process involves isothermal solidification, which occurs when copper and tin are in contact at temperatures up to 300C. This interaction promotes the growth of IMCs across the interface between the solder and the substrate, resulting in a bond that possesses a melting point higher than the bonding temperature itself. This characteristic is particularly advantageous for applications requiring robust thermal and mechanical performance, as it ensures reliable connections in high-temperature environments.\\n\\nIn addition to the inherent advantages of the Sn-Cu system, the incorporation of metal powder into the solder paste significantly influences the diffusion processes and overall effectiveness of TLP bonding technology. The presence of metal powder increases the reaction surface area, which accelerates the diffusion processes necessary for IMC formation. By shortening the diffusion path, the rate at which intermetallic phases develop is enhanced, leading to a more efficient bonding process. This is particularly critical in power electronics interconnections, where large joints, such as chip attach or chip stacking, require dependable and thermally stable connections.\\n\\nFurthermore, the strategic use of metal powder not only facilitates faster IMC formation but also contributes to the creation of a composite solder paste that can improve the thermal stability of the joint. The thermally stable intermetallic phases formed during the molten phase of the solder are a direct result of these diffusion processes, which are optimized by the presence of metal powder.\\n\\nIn summary, the combination of the Sn-Cu system's ability to form stable IMCs and the incorporation of metal powder to enhance diffusion processes positions TLP soldering as a promising technology for high-performance electronic applications. This synergy not only improves the thermal stability of the joints but also ensures their mechanical integrity under demanding operational conditions, making it a vital approach in the advancement of power electronics interconnection technology.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(formatted_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Model Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map = {\"\" :0}\n",
    ").to(device)\n",
    "\n",
    "'''if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    quant_model = torch.nn.DataParallel(quant_model)'''\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen 2.5 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(quant_model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 1081344 || Total Parameters: 316200832, || % trainable: 0.34198012483408013\n"
     ]
    }
   ],
   "source": [
    "def net_trainable_params(model):    #prints the net trainable parameters for fine-tuning \n",
    "    all_params = 0\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad == True:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"Trainable Parameters: {trainable_params} || Total Parameters: {all_params}, || % trainable: {100*trainable_params/all_params}\")\n",
    "    \n",
    "net_trainable_params(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/546 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 546/546 [00:00<00:00, 3015.70 examples/s]\n",
      "Map: 100%|| 61/61 [00:00<00:00, 2399.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    return {key: val.to(device) for key, val in tokens.items()}  # Move tokens to GPU\n",
    "\n",
    "    #return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_train_dataset = formatted_dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = formatted_dataset[\"test\"].map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 546/546 [00:00<00:00, 10798.59 examples/s]\n",
      "Map: 100%|| 61/61 [00:00<00:00, 6947.79 examples/s]\n",
      "Map: 100%|| 546/546 [00:00<00:00, 4091.73 examples/s]\n",
      "Map: 100%|| 61/61 [00:00<00:00, 3205.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Move Dataset to the same GPU\n",
    "def move_to_gpu(example):\n",
    "    return {key: torch.tensor(val).to(device) if isinstance(val, list) else val for key, val in example.items()}\n",
    "\n",
    "formatted_dataset[\"train\"] = formatted_dataset[\"train\"].map(move_to_gpu)\n",
    "formatted_dataset[\"test\"] = formatted_dataset[\"test\"].map(move_to_gpu)\n",
    "\n",
    "\"\"\"if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    quant_model = torch.nn.DataParallel(quant_model)\"\"\"\n",
    "\n",
    "supervised_finetuning_trainer = SFTTrainer(\n",
    "    peft_model,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        max_steps=5000,\n",
    "        output_dir=\"./SMT-LLM\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        fp16=True,\n",
    "        dataset_text_field=\"text\",\n",
    "        num_train_epochs=2\n",
    "        \n",
    "    ),\n",
    "    processing_class=tokenizer,\n",
    "    #tokenizer=tokenizer, #depricated \n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    #max_seq_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kicking off the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/peft/peft_model.py\", line 1745, in forward\n    return self.base_model(\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1165, in forward\n    outputs = self.model(\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 895, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 623, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 501, in forward\n    query_states = self.q_proj(hidden_states)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n    result = self.base_layer(x, *args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n    return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/autograd/function.py\", line 598, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1362, in dequantize_4bit\n    is_on_gpu([A, absmax, out])\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 469, in is_on_gpu\n    raise RuntimeError(\nRuntimeError: Input tensors need to be on the same GPU, but found the following tensor and device combinations:\n [(torch.Size([1, 401408]), device(type='cuda', index=1)), (torch.Size([12544]), device(type='cuda', index=0)), (torch.Size([896, 896]), device(type='cuda', index=1))]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msupervised_finetuning_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3660\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3708\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3706\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3707\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3708\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3710\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/peft/peft_model.py\", line 1745, in forward\n    return self.base_model(\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1165, in forward\n    outputs = self.model(\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 895, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 623, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 501, in forward\n    query_states = self.q_proj(hidden_states)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n    result = self.base_layer(x, *args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n    return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/torch/autograd/function.py\", line 598, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1362, in dequantize_4bit\n    is_on_gpu([A, absmax, out])\n  File \"/home/ritwik/.local/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 469, in is_on_gpu\n    raise RuntimeError(\nRuntimeError: Input tensors need to be on the same GPU, but found the following tensor and device combinations:\n [(torch.Size([1, 401408]), device(type='cuda', index=1)), (torch.Size([12544]), device(type='cuda', index=0)), (torch.Size([896, 896]), device(type='cuda', index=1))]\n"
     ]
    }
   ],
   "source": [
    "supervised_finetuning_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing the Weights to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|| 1.98G/1.98G [01:02<00:00, 31.5MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned/commit/7c0c2a8e908fd4fea6772874f51bd4bbf95202af', commit_message='Upload Qwen2ForCausalLM', commit_description='', oid='7c0c2a8e908fd4fea6772874f51bd4bbf95202af', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned', endpoint='https://huggingface.co', repo_type='model', repo_id='ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer.json: 100%|| 11.4M/11.4M [00:00<00:00, 25.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned_tokenizer/commit/45f194377940a70527713ba6c7aea04a64a30213', commit_message='Upload tokenizer', commit_description='', oid='45f194377940a70527713ba6c7aea04a64a30213', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned_tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned were not used when initializing Qwen2ForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.bias', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.bias', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.bias', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.bias', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.bias', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.bias', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.bias', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.bias', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.bias', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.bias', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.bias', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.bias', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.bias', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.bias', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.bias', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.bias', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.bias', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.bias', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.bias', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.bias', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.bias', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.bias', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.bias', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.bias', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.bias', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.bias', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.bias', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.bias', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.bias', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.bias', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.bias', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.bias', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.bias', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.bias', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.bias', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.bias', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.bias', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.bias', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.bias', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.bias', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.bias', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.bias', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.bias', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.bias', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.bias', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.bias', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.bias', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.bias', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned and are newly initialized: ['model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "#lora_config = LoraConfig.from_pretrained(\"ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned_tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"ritwik-ghosh/Qwen2.5_0.5B_SMT_Fully-fine-tuned\",\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id).to(\"cuda:0\")\n",
    "\n",
    "def make_inference(instruction, context = None):\n",
    "  if context:\n",
    "    prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "  else:\n",
    "    prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "  \n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda:0\")\n",
    "  base_outputs = base_model.generate(**inputs, max_new_tokens=100)\n",
    "  display(Markdown((tokenizer.decode(base_outputs[0], skip_special_tokens=True))))\n",
    "  \n",
    "  tuned_outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "  print(\"---- NON-INSTRUCT-TUNED-MODEL ----\")\n",
    "  display(Markdown((tokenizer.decode(tuned_outputs[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction: \n",
       "How to accurately predict the surface tension of a lead-free solder?\n",
       "\n",
       "### Response: \n",
       "To accurately predict the surface tension of a lead-free solder, you can follow these steps:\n",
       "\n",
       "1. **Determine the composition of the solder**: The surface tension of a lead-free solder is influenced by its composition. Lead-free solder typically contains lead, tin, and other metals. The specific composition will determine the surface tension.\n",
       "\n",
       "2. **Measure the surface tension**: Use a surface tension meter or a specialized tool to measure the surface tension of the solder. This will give you a precise value for the"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- NON-INSTRUCT-TUNED-MODEL ----\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction: \n",
       "How to accurately predict the surface tension of a lead-free solder?\n",
       "\n",
       "### Response: \n",
       " ( ( ( ( ( ( ( ( ornonen---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_inference(instruction=\"How to accurately predict the surface tension of a lead-free solder?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
